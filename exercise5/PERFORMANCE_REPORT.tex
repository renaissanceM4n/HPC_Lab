\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{hyperref}

\graphicspath{{./}{original_implementation/screenshots/}{original_implementation/screenshots/linaroForge_map/}{original_implementation/screenshots/vampir/}{hybrid_implementation/screenshots/}}

\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    showstringspaces=false
}

\title{Exercise 5: Profiling and Tracing the Snowman Ray Tracer\\Original vs. Hybrid Renderer}
\author{David Peitz}
\date{\today}

\begin{document}

\maketitle

\section{Scope and Goals}

This report profiles and generates traces for the original (non-hybrid) and hybrid Snowman renderer using the tools discussed in the lab. The analysis is structured to first use Linaro MAP (sample-based profiling) to find hotspots, and then use Score-P (instrumentation + tracing) with Vampir (trace visualization) to explain time-resolved behavior. The main goals are:

\begin{itemize}
\item Identify the most expensive functions/regions.
\item Identify bottlenecks (e.g., synchronization, load imbalance, communication).
\item Compare original vs. hybrid execution while keeping the same total number of workers.
\end{itemize}

\section{Experimental Setup}

\subsection{Software and Tools}
\begin{itemize}
\item OpenMPI 4.1.6 (GCC 13.2.0 toolchain)
\item Linaro Forge / MAP 25.0.3
\item Score-P 8.4 (tracing + filtering)
\item Vampir (timeline visualization)
\end{itemize}

\subsection{Workload and Configurations}

The workload is the Snowman scene rendering with image size $1024\times 1024$ and 4 snowmen.

To ensure a fair comparison, both implementations are compared using the same total number of workers:
\begin{itemize}
\item \textbf{Original:} 96 MPI ranks $\times$ 1 thread $=96$ workers.
\item \textbf{Hybrid:} 24 MPI ranks $\times$ 4 OpenMP threads $=96$ workers.
\end{itemize}

Additionally, the original implementation is profiled at 60 MPI ranks (besides 96) to illustrate how the dominant cost shifts when reducing the MPI rank count.

\paragraph{Why 24 MPI ranks $\times$ 4 threads?}
In the hybrid renderer, OpenMP parallelism is applied to the nested pixel loop over each tile. For a tile of size $T\times T$, the loop provides $T^2$ independent pixel iterations. If the number of OpenMP threads is increased while keeping the MPI process count small, the available parallel work per process can become too fine-grained.

For example, for a $16\times 16$ tile there are $16^2 = 256$ pixel iterations. With 24 OpenMP threads, this yields only $256/24 \approx 10$--11 iterations per thread. In this regime, the per-thread work is too small to amortize OpenMP scheduling and synchronization costs (e.g., loop scheduling and implicit barriers), resulting in poor efficiency.

Therefore, this report uses a fixed thread count of 4 and increases the number of MPI processes: when you fix the number of threads to 4 and increase the number of processes, more tiles are processed concurrently and each process performs a sufficiently large chunk of work. For the same $16\times 16$ example, 4 threads yield $256/4 = 64$ iterations per thread, which reduces OpenMP overhead and typically improves cache behavior due to more contiguous work per thread.

\section{Linaro MAP Profiling Results}

This section summarizes MAP's sampling-based profiling for the original and hybrid implementations.

\subsection{Original Implementation (60 and 96 MPI ranks)}

MAP's sampling highlights that the dominant cost depends strongly on the MPI rank count.

\paragraph{Example 1: 96 MPI ranks --- \texttt{MPI\_Finalize()} dominates}
At 96 ranks, the MAP source view shows that a large fraction of time is attributed to the line containing \texttt{MPI\_Finalize()}. This indicates that ranks spend substantial time inside the MPI library at shutdown, which typically means \textbf{waiting/synchronization} (e.g., stragglers reaching the end later than others).

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{map_mpi_finalize_p_96.png}
\caption{Original implementation (96 MPI ranks): \texttt{MPI\_Finalize()} dominates the sampled time. The line-level breakdown shows \emph{Calling functions} (time inside the called MPI routine), not \emph{Executing instructions} of user code.}
\label{fig:map_original_finalize_96}
\end{figure}

\paragraph{Calling functions vs. executing instructions}
MAP distinguishes whether samples on a source line are spent in the machine instructions of that line (\emph{executing instructions}) or in code reached by calling a function from that line (\emph{calling functions}).
For \texttt{MPI\_Finalize()}, the time is almost entirely classified as \emph{calling functions}, meaning the CPU time is spent inside the MPI runtime/library rather than executing user instructions on that source line. In practice, this usually corresponds to blocking behavior (global synchronization / waiting).

\paragraph{Example 2: 60 MPI ranks --- compute dominates (\texttt{render()})}
At 60 ranks, the MAP call site in \texttt{main()} identifies the call to \texttt{RayTracer::render(...)} as the dominant cost. The line-level breakdown is again primarily \emph{calling functions}, which is expected for a function call site: it means the time is spent inside \texttt{render()}, not in the call instruction itself.

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{map_render_p_60.png}
\caption{Original implementation (60 MPI ranks): the main cost is the call to \texttt{RayTracer::render(...)} (compute-heavy phase).}
\label{fig:map_original_render_60}
\end{figure}

\paragraph{Drill-down into the compute kernel}
When drilling down into \texttt{render()}, MAP points to a hotspot inside the ray tracer kernel (pixel/geometry work). Here the breakdown is dominated by \emph{executing instructions}, i.e., the CPU is spending time executing the concrete instructions at that source location.
This suggests that for 60 ranks, the limiting factor is the compute kernel itself (branch-heavy intersection logic and memory-access patterns) rather than MPI synchronization.

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{map_pixel_loop_p_60.png}
\caption{Original implementation (60 MPI ranks): inside the ray tracer kernel the hotspot is dominated by \emph{executing instructions}, indicating instruction-level work (branches/memory accesses) rather than call/wait overhead.}
\label{fig:map_original_pixel_60}
\end{figure}

\subsection{Hybrid Implementation (24 MPI ranks $\times$ 4 threads)}

For the hybrid configuration, MAP attributes the majority of time to the OpenMP-parallelized pixel loop inside the ray tracing kernel. In the ``Accumulated Exclusive Time per Function'' view the hottest region corresponds to the OpenMP loop (shown as \texttt{\$omp for @raytracer.cpp:325}), while MPI calls such as \texttt{MPI\_Recv} contribute comparatively little.

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{accumulated_time_per_function_p_24.png}
\caption{Hybrid implementation (24 MPI ranks $\times$ 4 threads): accumulated exclusive time per function.}
\label{fig:map_hybrid_accum}
\end{figure}

This is consistent with the intended design: MPI is used for distributing tiles/results, while most wall time is spent in the compute-heavy per-pixel ray traversal and intersection tests.

The large cost of the OpenMP loop is explained directly by the computation performed per pixel in \texttt{RayTracer::renderTile()}. For each pixel, the code (i) computes a primary ray direction through the pixel, (ii) tests the ray against all spheres and all planes to find the closest intersection, (iii) evaluates shading including surface normal computation and shadow checks (casting a ``shadow ray'' and iterating over scene geometry to determine occlusion), and (iv) applies a snowflake overlay by iterating over a large set of snowflake points (\texttt{snowflake\_count = 75000}) and checking whether the ray passes sufficiently close to any snowflake. This combination results in a high arithmetic intensity and a high number of per-pixel loop iterations, which naturally dominates runtime when MPI overhead is reduced.

\paragraph{Line-level hotspot (sampling)}
MAP is a sampling profiler: the most frequently sampled source location typically indicates where CPU time is spent. In this case, the sampling hotspot aligns with the OpenMP-parallelized pixel loop in the ray tracer (the loop annotated as \texttt{\$omp for @raytracer.cpp:325}), matching the expectation that the pixel work dominates compute time.

\section{Score-P Tracing and Vampir Visualization}

Score-P tracing provides a time-resolved view of execution that helps distinguish compute from synchronization and communication.

\subsection{Original Implementation: MPI-dominated timelines}

The Vampir timelines for the original implementation show large contiguous regions of MPI management/collectives, with long phases where ranks are not executing user computation. This is consistent with the MAP finding at 96 MPI ranks that \texttt{MPI\_Finalize()} dominates: the end of the program is dominated by synchronization and waiting.

In addition, Vampir's ``Accumulated Exclusive Time per Function'' view clearly highlights that \texttt{MPI\_Finalize} is the largest contributor, followed by \texttt{MPI\_Init} and collectives such as \texttt{MPI\_Reduce} and \texttt{MPI\_Gatherv}. Note that these accumulated values are summed over all ranks; a large accumulated value therefore typically indicates many ranks spending a non-trivial amount of time inside that MPI routine (often blocked).

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{accumulated_time_per_function_p_96.png}
\caption{Original implementation (96 MPI ranks): accumulated exclusive time per function (summed over ranks). \texttt{MPI\_Finalize} dominates.}
\label{fig:vampir_original_accum}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{summary_timeline_p_96.png}
\caption{Original implementation (96 MPI ranks): Vampir summary timeline.}
\label{fig:vampir_original_summary}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{master_timeline_p_96.png}
\caption{Original implementation (96 MPI ranks): Vampir master timeline.}
\label{fig:vampir_original_master}
\end{figure}

The timeline is consistent with a two-part end phase: a long collective communication step and then long time in MPI shutdown. The master-thread inspection shows long contiguous intervals for \texttt{MPI\_Gatherv} and \texttt{MPI\_Finalize} (both on the order of $\sim 38$s in the provided view), which strongly suggests that the program is not limited by per-rank compute at the end, but by global synchronization/tail effects (e.g., waiting for slow ranks before finalization completes).

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{master_timeline_description_master_thread_p_96.png}
\caption{Original implementation (96 MPI ranks): master-thread detail view showing a long \texttt{MPI\_Gatherv} interval.}
\label{fig:vampir_original_master_detail_gatherv}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{master_timeline_description_worker_thread_p_96.png}
\caption{Original implementation (96 MPI ranks): worker-thread detail view showing a long \texttt{MPI\_Finalize} interval.}
\label{fig:vampir_original_worker_detail_finalize}
\end{figure}

\subsection{Hybrid Implementation: compute-dominated timelines}

For the hybrid renderer, the Vampir timelines show that the main phase is dominated by the OpenMP compute region (pixel loop), and MPI accounts for only a small fraction of runtime. The timeline shows near-continuous OpenMP activity across threads during the compute phase, which matches the MAP result identifying the OpenMP loop as the hottest region.

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{summary_timeline_p_24.png}
\caption{Hybrid implementation (24 MPI ranks $\times$ 4 threads): Vampir summary timeline.}
\label{fig:vampir_hybrid_summary}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{master_timeline_p_24.png}
\caption{Hybrid implementation (24 MPI ranks $\times$ 4 threads): Vampir master timeline.}
\label{fig:vampir_hybrid_master}
\end{figure}

\section{Comparative Analysis: Original vs. Hybrid}

\paragraph{Original (96 ranks): bottleneck is synchronization/imbalance.}
At 96 MPI ranks, profiling and tracing consistently indicate that MPI dominates runtime. The observed spread in per-rank compute times implies substantial load imbalance, which manifests as long waiting phases in collectives and finalize. As a result, adding more MPI ranks does not translate into higher throughput for this configuration.

\paragraph{Hybrid (24$\times$4): bottleneck is pixel computation.}
The hybrid configuration shifts the bottleneck back to the compute kernel: the OpenMP-parallel pixel loop dominates both MAP and Vampir views, while MPI time is relatively small. This indicates that the hybrid design reduces global MPI synchronization overhead (for the same total worker count) and uses shared-memory parallelism effectively within tiles.

\paragraph{Most expensive regions/functions (high-level).}
\begin{itemize}
\item \textbf{Original:} MPI collectives/management (\texttt{MPI\_Finalize}, \texttt{MPI\_Gatherv}, \texttt{MPI\_Reduce}).
\item \textbf{Hybrid:} OpenMP pixel loop in the ray tracer (\texttt{\$omp for @raytracer.cpp:325}).
\end{itemize}

\section{Conclusions}

\begin{enumerate}
\item Increasing the MPI rank count for the original renderer (60 $\rightarrow$ 96) shifts the runtime towards MPI overhead, and timelines indicate waiting in collectives/finalize, consistent with load imbalance.
\item The hybrid configuration with 24 MPI ranks and 4 OpenMP threads (96 workers total) makes the pixel computation the dominant cost again, with MPI overhead comparatively small.
\item The combined MAP + Score-P/Vampir workflow is effective: MAP identifies hotspots (functions/lines), while Vampir timelines explain whether time is spent computing or waiting.
\end{enumerate}

\end{document}
