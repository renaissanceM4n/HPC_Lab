Command:        mpirun -n 48 ./snowman 1024 4
Resources:      1 node (96 physical, 96 logical cores per node)
Memory:         1008 GiB per node
Tasks:          48 processes
Machine:        node019
Architecture:   x86_64
CPU Family:     sapphirerapids
Start time:     Sat Nov 15 13:40:46 2025
Total time:     15 seconds
Full path:      /home/s13dpeit_hpc/Snowman/HPC_Lab

Summary: snowman is Compute-bound in this configuration
Compute:                                     96.5%    (14.3s) |=========|
MPI:                                          3.5%     (0.5s) ||
I/O:                                         <0.1%     (0.0s) ||
This application run was Compute-bound (based on main thread activity). A breakdown of this time and advice for investigating further is in the CPU section below.
As very little time is spent in MPI calls, this code may also benefit from running at larger scales.

CPU:
A breakdown of the 96.5% (14.3s) CPU time:
Scalar numeric ops:                          54.0%     (7.7s) |====|
Vector numeric ops:                          <0.1%     (0.0s) ||
Memory accesses:                             30.4%     (4.3s) |==|
The per-core performance is arithmetic-bound. Try to increase the amount of time spent in vectorized instructions by analyzing the compiler's vectorization reports.

MPI:
A breakdown of the 3.5% (0.5s) MPI time:
Time in collective calls:                    39.8%     (0.2s) |===|
Time in point-to-point calls:                60.2%     (0.3s) |=====|
Effective process collective rate:             116 bytes/s
Effective process point-to-point rate:         425 kB/s
Most of the time is spent in point-to-point calls with a very low transfer rate. This suggests load imbalance is causing synchronization overhead; use an MPI profiler to investigate.
The collective transfer rate is very low. This suggests load imbalance is causing synchronization overhead; use an MPI profiler to investigate.

I/O:
A breakdown of the <0.1% (0.0s) I/O time:
Time in reads:                                0.0%     (0.0s) |
Time in writes:                             100.0%     (0.0s) |=========|
Effective process read rate:                  0.00 bytes/s
Effective process write rate:                  160 MB/s
Most of the time is spent in write operations with an average effective transfer rate. It may be possible to achieve faster effective transfer rates using asynchronous file operations.

Threads:
A breakdown of how multiple threads were used:
Computation:                                100.0%  (1476.9s) |=========|
Synchronization:                              0.0%     (0.0s) |
Physical core utilization:                   50.0%            |====|
System load:                                 50.1%            |====|
Physical core utilization is low. Try increasing the number of threads or processes to improve performance.

Memory:
Per-process memory usage may also affect scaling:
Mean process memory usage:                     184 MiB
Peak process memory usage:                     189 MiB
Peak node memory usage:                       2.0%            ||
The peak node memory usage is very low. Running with fewer MPI processes and more data on each process may be more efficient.

Energy:
A breakdown of how energy was used:
CPU:                                      not supported
System:                                   not supported
Mean node power:                          not supported
Peak node power:                              0.00 W
Energy metrics are not available on this system.
CPU metrics: Error reading /sys/class/powercap/intel-rapl:0/energy_uj: Permission denied

Thread Affinity:
A breakdown of how software threads have been pinned to logical cores (1 per physical core).
Mean utilization:                           100.0%            |=========|
Max load:                                     12.0 
Migration opportunity:                        12.0 
[ERROR] 48 processes have overlapping affinity masks e.g. ranks 0 and 24
[ERROR] detected compute threads with overlapping affinity masks
[INFORMATION] consider improving node utilization by running 8 processes per node (1 process per NUMANode).
Consult Linaro MAP's Thread Affinity Advisor dialog for more details.

