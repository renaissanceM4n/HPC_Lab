Command:        mpirun -n 48 ./snowman 1024 4
Resources:      1 node (96 physical, 96 logical cores per node)
Memory:         1008 GiB per node
Tasks:          48 processes
Machine:        node170
Architecture:   x86_64
CPU Family:     sapphirerapids
Start time:     Sat Nov 15 17:32:44 2025
Total time:     23 seconds
Full path:      /home/s13dpeit_hpc/Snowman

Summary: snowman is Compute-bound in this configuration
Compute:                                     62.3%    (14.2s) |=====|
MPI:                                         37.7%     (8.6s) |===|
I/O:                                          0.0%     (0.0s) |
This application run was Compute-bound (based on main thread activity). A breakdown of this time and advice for investigating further is in the CPU section below.

CPU:
A breakdown of the 62.3% (14.2s) CPU time:
Scalar numeric ops:                          57.6%     (8.2s) |=====|
Vector numeric ops:                           0.0%     (0.0s) |
Memory accesses:                             32.2%     (4.6s) |==|
The per-core performance is arithmetic-bound. Try to increase the amount of time spent in vectorized instructions by analyzing the compiler's vectorization reports.

MPI:
A breakdown of the 37.7% (8.6s) MPI time:
Time in collective calls:                   100.0%     (8.6s) |=========|
Time in point-to-point calls:                 0.0%     (0.0s) |
Effective process collective rate:            15.2 kB/s
Effective process point-to-point rate:        0.00 bytes/s

I/O:
A breakdown of the 0.0% (0.0s) I/O time:
Time in reads:                                0.0%     (0.0s) |
Time in writes:                               0.0%     (0.0s) |
Effective process read rate:                  0.00 bytes/s
Effective process write rate:                 0.00 bytes/s
No time is spent in I/O operations. There's nothing to optimize here!

Threads:
A breakdown of how multiple threads were used:
Computation:                                100.0%  (2279.7s) |=========|
Synchronization:                              0.0%     (0.0s) |
Physical core utilization:                   50.0%            |====|
System load:                                 50.4%            |====|
Physical core utilization is low. Try increasing the number of threads or processes to improve performance.

Memory:
Per-process memory usage may also affect scaling:
Mean process memory usage:                     183 MiB
Peak process memory usage:                     189 MiB
Peak node memory usage:                       2.0%            ||
The peak node memory usage is very low. Running with fewer MPI processes and more data on each process may be more efficient.

Energy:
A breakdown of how energy was used:
CPU:                                      not supported
System:                                   not supported
Mean node power:                          not supported
Peak node power:                              0.00 W
Energy metrics are not available on this system.
CPU metrics: Error reading /sys/class/powercap/intel-rapl:0/energy_uj: Permission denied

Thread Affinity:
A breakdown of how software threads have been pinned to logical cores (1 per physical core).
Mean utilization:                           100.0%            |=========|
Max load:                                     12.0 
Migration opportunity:                        12.0 
[ERROR] 48 processes have overlapping affinity masks e.g. ranks 19 and 27
[ERROR] detected compute threads with overlapping affinity masks
[INFORMATION] consider improving node utilization by running 8 processes per node (1 process per NUMANode).
Consult Linaro MAP's Thread Affinity Advisor dialog for more details.

