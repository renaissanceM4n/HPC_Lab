Command:        mpirun -n 96 ./snowman 1024 4
Resources:      1 node (96 physical, 96 logical cores per node)
Memory:         1008 GiB per node
Tasks:          96 processes
Machine:        node001
Architecture:   x86_64
CPU Family:     sapphirerapids
Start time:     Wed Nov 5 19:45:22 2025
Total time:     49 seconds
Full path:      /home/s13dpeit_hpc/Snowman

Summary: snowman is MPI-bound in this configuration
Compute:                                     14.9%     (7.2s) ||
MPI:                                         85.1%    (41.0s) |========|
I/O:                                          0.0%     (0.0s) |
This application run was MPI-bound. A breakdown of this time and advice for investigating further is in the MPI section below.

CPU:
A breakdown of the 14.9% (7.2s) CPU time:
Scalar numeric ops:                          62.8%     (4.5s) |=====|
Vector numeric ops:                           0.0%     (0.0s) |
Memory accesses:                             34.7%     (2.5s) |==|
The per-core performance is arithmetic-bound. Try to increase the amount of time spent in vectorized instructions by analyzing the compiler's vectorization reports.
Significant time is spent on memory accesses. Use a profiler to identify time-consuming loops and check their cache performance.

MPI:
A breakdown of the 85.1% (41.0s) MPI time:
Time in collective calls:                   100.0%    (41.0s) |=========|
Time in point-to-point calls:                 0.0%     (0.0s) |
Effective process collective rate:            1.66 kB/s
Effective process point-to-point rate:        0.00 bytes/s

I/O:
A breakdown of the 0.0% (0.0s) I/O time:
Time in reads:                                0.0%     (0.0s) |
Time in writes:                               0.0%     (0.0s) |
Effective process read rate:                  0.00 bytes/s
Effective process write rate:                 0.00 bytes/s
No time is spent in I/O operations. There's nothing to optimize here!

Threads:
A breakdown of how multiple threads were used:
Computation:                                  0.0%     (0.0s) |
Synchronization:                              0.0%     (0.0s) |
Physical core utilization:                  100.0%            |=========|
System load:                                101.0%            |=========|
No measurable time is spent in multithreaded code.

Memory:
Per-process memory usage may also affect scaling:
Mean process memory usage:                     187 MiB
Peak process memory usage:                     193 MiB
Peak node memory usage:                       3.0%            ||
The peak node memory usage is very low. Running with fewer MPI processes and more data on each process may be more efficient.

Energy:
A breakdown of how energy was used:
CPU:                                      not supported
System:                                   not supported
Mean node power:                          not supported
Peak node power:                              0.00 W
Energy metrics are not available on this system.
CPU metrics: Error reading /sys/class/powercap/intel-rapl:0/energy_uj: Permission denied

Thread Affinity:
A breakdown of how software threads have been pinned to logical cores (1 per physical core).
Mean utilization:                           100.0%            |=========|
Max load:                                     24.0 
Migration opportunity:                        12.0 
[ERROR] 96 processes have overlapping affinity masks e.g. ranks 15 and 95
[ERROR] detected compute threads with overlapping affinity masks
[ERROR] cores are oversubscribed
Consult Linaro MAP's Thread Affinity Advisor dialog for more details.

